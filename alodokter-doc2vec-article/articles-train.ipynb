{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install numpy pandas matplotlib sklearn seaborn\n",
    "# !{sys.executable} -m pip install --upgrade gensim\n",
    "# !{sys.executable} -m pip install nltk\n",
    "# !{sys.executable} -m pip install beautifulsoup4\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import multiprocessing\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_FILES = \"\"\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\"\"\"\n",
    "read training files\n",
    "\"\"\"\n",
    "def read_files(path):\n",
    "    print(\"path: {}...\".format(path))\n",
    "    for root, dirnames, filenames in os.walk(path):\n",
    "        for dirname in dirnames:\n",
    "            read_files(os.path.join(root, dirname))\n",
    "        for filename in filenames:\n",
    "            if filename not in SKIP_FILES:\n",
    "                filepath = os.path.join(root, filename)\n",
    "                if os.path.isfile(filepath):\n",
    "                    lines = []\n",
    "                    f = open(filepath, encoding='latin-1')\n",
    "                    for line in f:\n",
    "                        lines.append(line)\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield filename, content\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\"Download a file if not present\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"downloading {}...\".format(filename))\n",
    "        filename, _ = request.urlretrieve(url + filename, filename)\n",
    "\n",
    "        print(\"extracting {}...\".format(filename))\n",
    "        with zipfile.ZipFile(filename) as f:\n",
    "            f.extractall()\n",
    "\n",
    "    \"\"\"directory data\"\"\"\n",
    "    data_path = filename.replace('.zip','')\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"extracting {}...\".format(filename))\n",
    "        with zipfile.ZipFile(filename) as f:\n",
    "            f.extractall()\n",
    "\n",
    "    \"\"\"directory model for saving model while training\"\"\"\n",
    "    if not os.path.exists('model'):\n",
    "        os.mkdir('model')\n",
    "        print(\"directory model created...\")\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, filename):\n",
    "    filename = download(url, filename)\n",
    "    data_path = filename.replace('.zip','')\n",
    "    documents = []\n",
    "    print(\"building documents...\")\n",
    "    for fname, text in read_files(data_path):\n",
    "        documents.append(clean_str(text).split(' '))\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]\n",
    "    print(\"building documents done\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting data.zip...\n",
      "building documents...\n",
      "path: data...\n",
      "building documents done\n",
      "we have 7932 documents\n"
     ]
    }
   ],
   "source": [
    "# download data training\n",
    "documents = get_data(url='https://github.com/kadriansyah/notebook/raw/master/alodokter-doc2vec-article/', filename=\"data.zip\")\n",
    "print(\"we have {} documents\".format(len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['penyebab', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'dan', 'cara', 'mengatasinya', 'bayi', 'muntah', 'setelah', 'minum', 'asi', '(', 'air', 'susu', 'ibu', ')', 'adalah', 'keluhan', 'yang', 'sering', 'terjadi', 'sebagian', 'bayi', 'bahkan', 'mengalaminya', 'hampir', 'setiap', 'kali', 'selesai', 'menyusu', 'meski', 'umumnya', 'normal', 'kondisi', 'ini', 'bisa', 'juga', 'disebabkan', 'oleh', 'gangguan', 'berbahaya', 'yang', 'harus', 'diwaspadai', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'dikenal', 'dengan', 'istilah', 'gumoh', 'gumoh', 'dikatakan', 'normal', 'apabila', 'tidak', 'menyebabkan', 'bayi', 'rewel', 'atau', 'sesak', 'napas', 'meskipun', 'dapat', 'dicegah', 'kondisi', 'tersebut', 'tidak', 'memerlukan', 'penanganan', 'khusus', 'dan', 'normal', 'terjadi', 'penyebab', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'gumoh', 'disebabkan', 'oleh', 'asi', 'atau', 'susu', 'yang', 'ditelan', 'bayi', 'kembali', 'ke', 'kerongkongan', 'karena', 'otot', 'di', 'saluran', 'pencernaan', 'bayi', 'yaitu', 'di', 'bagian', 'kerongkongan', 'dan', 'lambung', 'masih', 'lemah', 'kondisi', 'ini', 'disebut', 'sebagai', 'refluks', 'bayi', 'kemungkinan', 'mengalami', 'refluks', 'karena', 'ukuran', 'lambungnya', 'masih', 'sangat', 'kecil', 'sehingga', 'cepat', 'terisi', 'penuh', 'refluks', 'juga', 'terjadi', 'karena', 'katup', 'pada', 'kerongkongan', 'belum', 'sempurna', 'sehingga', 'belum', 'bekerja', 'secara', 'optimal', 'untuk', 'menahan', 'isi', 'lambung', 'umumnya', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'akan', 'berlangsung', 'hingga', 'usia', '4', '5', 'bulan', 'setelah', 'itu', 'gumoh', 'akan', 'berhenti', 'dengan', 'sendirinya', 'penyebab', 'lain', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'adalah', 'gastroenteritis', 'hanya', 'saja', 'infeksi', 'pada', 'saluran', 'cerna', 'bayi', 'ini', 'biasanya', 'disertai', 'dengan', 'diare', 'selain', 'gastoenteritis', 'ada', 'berbagai', 'penyebab', 'lain', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'mulai', 'dari', 'alergi', 'pilek', 'infeksi', 'telinga', 'infeksi', 'saluran', 'kemih', 'hingga', 'penyempitan', 'lambung', '(', 'stenosis', 'pilorus', ')', 'walaupun', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'sering', 'kali', 'disebabkan', 'oleh', 'gumoh', 'yang', 'normal', 'tapi', 'orang', 'tua', 'harus', 'tetap', 'mewaspadai', 'jika', 'bayi', 'muntah', 'disertai', 'dengan', 'gejala', 'lain', 'seperti', 'demam', 'kurang', 'mau', 'atau', 'tidak', 'mau', 'menyusu', 'sama', 'sekali', 'timbul', 'ruam', 'sulit', 'tidur', 'dan', 'rewel', 'ubun', 'ubun', 'tampak', 'menonjol', 'perut', 'bengkak', 'sesak', 'napas', 'muntah', 'disertai', 'darah', 'atau', 'cairan', 'hijau', 'muntah', 'terus', 'menerus', 'lebih', 'lebih', 'dari', 'satu', 'atau', 'dua', 'hari', 'mengalami', 'dehidrasi', 'yang', 'ditandai', 'dengan', 'bibir', 'kering', 'menangis', 'tanpa', 'air', 'mata', 'ubun', 'ubun', 'cekung', 'dan', 'jarang', 'buang', 'air', 'kecil', 'tips', 'meringankan', 'muntah', 'pada', 'bayi', 'bayi', 'gumoh', 'biasanya', 'tidak', 'perlu', 'dikhawatirkan', 'dan', 'akan', 'mereda', 'dengan', 'sendirinya', 'seiring', 'bertambahnya', 'usia', 'bayi', 'meski', 'demikian', 'ada', 'beberapa', 'cara', 'yang', 'dapat', 'dilakukan', 'untuk', 'meringankan', 'keluhan', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'upayakan', 'posisi', 'kepala', 'bayi', 'lebih', 'tinggi', 'dari', 'tubuhnya', 'saat', 'menyusu', 'posisikan', 'tubuhnya', 'tetap', 'tegak', 'setelah', 'menyusu', 'agar', 'bayi', 'dapat', 'lebih', 'mudah', 'bersendawa', 'biarkan', 'bayi', 'menyusu', 'dalam', 'keadaan', 'tenang', 'hal', 'ini', 'akan', 'mencegah', 'bayi', 'mengisap', 'terlalu', 'banyak', 'udara', 'bersamaan', 'dengan', 'asi', 'biasakan', 'bayi', 'menyusu', 'secukupnya', 'namun', 'lebih', 'sering', 'menyusu', 'terlalu', 'banyak', 'dapat', 'membuat', 'lambung', 'bayi', 'teregang', 'karena', 'penuh', 'sehingga', 'memicu', 'bayi', 'untuk', 'muntah', 'setelah', 'minum', 'asi', 'buat', 'bayi', 'sendawa', 'setiap', 'kali', 'habis', 'menyusu', 'biarkan', 'bayi', 'sendawa', 'terlebih', 'dulu', 'sebelum', 'berganti', 'payudara', 'pastikan', 'pakaian', 'atau', 'popok', 'bayi', 'tidak', 'terlalu', 'ketat', 'serta', 'hindari', 'menggendong', 'bayi', 'untuk', 'sendawa', 'dengan', 'posisi', 'perut', 'bayi', 'tepat', 'di', 'bahu', 'anda', 'hal', 'ini', 'untuk', 'mengurangi', 'tekanan', 'pada', 'perutnya', 'hindari', 'menggoyangkan', 'bayi', 'atau', 'membuat', 'bayi', 'aktif', 'segera', 'setelah', 'menyusu', 'sebaiknya', 'juga', 'jangan', 'bepergian', 'dengan', 'kendaraan', 'sesaat', 'setelah', 'bayi', 'menyusu', 'jika', 'bayi', 'sudah', 'cukup', 'besar', 'posisikan', 'agar', 'ia', 'duduk', 'sekitar', '30', 'menit', 'setelah', 'menyusu', 'posisikan', 'kepala', 'bayi', 'sedikit', 'lebih', 'tinggi', 'saat', 'tidur', 'anda', 'dapat', 'meletakkan', 'selimut', 'atau', 'handuk', 'yang', 'digulung', 'di', 'bawah', 'bahu', 'dan', 'kepalanya', 'sebaiknya', 'hindari', 'menggunakan', 'bantal', 'pada', 'bayi', 'teliti', 'kemungkinan', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'akibat', 'makanan', 'atau', 'minuman', 'yang', 'dikonsumsi', 'ibu', 'misalnya', 'susu', 'sapi', 'jika', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'disertai', 'tanda', 'tanda', 'bahaya', 'di', 'atas', 'atau', 'jika', 'anda', 'merasa', 'khawatir', 'akan', 'kondisi', 'ini', 'segeralah', 'konsultasikan', 'dengan', 'dokter', 'anak', 'catat', 'berapa', 'kali', 'atau', 'berapa', 'banyak', 'bayi', 'muntah', 'dan', 'apakah', 'terdapat', 'gejala', 'gejala', 'lainnya'], tags=[0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, documents, steps):\n",
    "    percentiles = np.zeros(steps)\n",
    "    for step in range(steps):\n",
    "        docid = np.random.randint(model.docvecs.count)\n",
    "        inferred_vector = model.infer_vector(documents[docid][0])\n",
    "        similars = model.docvecs.most_similar(positive=[inferred_vector], topn=10)\n",
    "        for idx,simdoc in enumerate(similars):\n",
    "            if simdoc[0] == docid:\n",
    "                print(\"found similar document with id {} in position {} with similarity score {}\".format(simdoc[0], idx, simdoc[1]))\n",
    "                percentiles[step] = ((len(similars) - idx) / len(similars)) * 100\n",
    "                break\n",
    "    return np.mean(percentiles)\n",
    "\n",
    "def train(documents=documents, model_name=\"model/alodokter-articles-doc2vec.model\", max_epochs=50, patience=3):\n",
    "    best_mean_percentiles = 0\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = Doc2Vec(dm=1, vector_size=300, window=2, alpha=0.1, min_alpha=0.0001, min_count=5, epochs=1, workers=5)\n",
    "    model.build_vocab(documents)\n",
    "    for epoch in range(max_epochs):\n",
    "        print('training epoch {:d} ...'.format(epoch))\n",
    "        model.train(documents, total_examples=model.corpus_count,epochs=model.epochs)\n",
    "        mean_percentiles = evaluate(model,documents,10)\n",
    "        print('mean percentiles: {:.2f}'.format(mean_percentiles))\n",
    "        \n",
    "        if mean_percentiles < best_mean_percentiles:\n",
    "            print(\"current mean_percentiles: {:.2f}, best: {:.2f}\".format(mean_percentiles, best_mean_percentiles))\n",
    "            patience = patience-1\n",
    "        else:\n",
    "            best_mean_percentiles = mean_percentiles\n",
    "            print(\"========== Saving best model with mean_percentiles: {:.2f} ==========\".format(mean_percentiles))\n",
    "            model.save(model_name)\n",
    "            patience = patience+1\n",
    "        \n",
    "        if patience == 0:\n",
    "            print(\"early stop...\")\n",
    "            print(\"========== Saving best model with mean_percentiles: {:.2f} ==========\".format(best_mean_percentiles))\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 09:52:51,811 : INFO : collecting all words and their counts\n",
      "2020-02-23 09:52:51,812 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-02-23 09:52:52,469 : INFO : collected 54008 word types and 7932 unique tags from a corpus of 7932 examples and 4866436 words\n",
      "2020-02-23 09:52:52,470 : INFO : Loading a fresh vocabulary\n",
      "2020-02-23 09:52:52,505 : INFO : effective_min_count=5 retains 15409 unique words (28% of original 54008, drops 38599)\n",
      "2020-02-23 09:52:52,505 : INFO : effective_min_count=5 leaves 4811064 word corpus (98% of original 4866436, drops 55372)\n",
      "2020-02-23 09:52:52,549 : INFO : deleting the raw counts dictionary of 54008 items\n",
      "2020-02-23 09:52:52,551 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2020-02-23 09:52:52,551 : INFO : downsampling leaves estimated 4105293 word corpus (85.3% of prior 4811064)\n",
      "2020-02-23 09:52:52,595 : INFO : estimated required memory for 15409 words and 300 dimensions: 54204500 bytes\n",
      "2020-02-23 09:52:52,596 : INFO : resetting layer weights\n",
      "2020-02-23 09:52:56,224 : INFO : training model with 5 workers on 15409 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 09:52:57,234 : INFO : EPOCH 1 - PROGRESS: at 37.10% examples, 1520374 words/s, in_qsize 10, out_qsize 1\n",
      "2020-02-23 09:52:58,238 : INFO : EPOCH 1 - PROGRESS: at 75.86% examples, 1551284 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 09:52:58,863 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-02-23 09:52:58,866 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 09:52:58,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 09:52:58,872 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 09:52:58,873 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 09:52:58,873 : INFO : EPOCH - 1 : training on 4866436 raw words (4113376 effective words) took 2.6s, 1554339 effective words/s\n",
      "2020-02-23 09:52:58,874 : INFO : training on a 4866436 raw words (4113376 effective words) took 2.6s, 1552905 effective words/s\n",
      "2020-02-23 09:52:58,875 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-02-23 09:52:58,916 : INFO : saving Doc2Vec object under model/alodokter-articles-doc2vec.model, separately None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found similar document with id 3220 in position 0 with similarity score 0.6797736883163452\n",
      "found similar document with id 5805 in position 0 with similarity score 0.7531086206436157\n",
      "found similar document with id 4936 in position 0 with similarity score 0.6532344222068787\n",
      "found similar document with id 2180 in position 0 with similarity score 0.695888876914978\n",
      "found similar document with id 3914 in position 0 with similarity score 0.6866844296455383\n",
      "found similar document with id 5740 in position 0 with similarity score 0.755409836769104\n",
      "found similar document with id 6122 in position 0 with similarity score 0.6206434965133667\n",
      "found similar document with id 2444 in position 0 with similarity score 0.7208614945411682\n",
      "found similar document with id 3624 in position 5 with similarity score 0.5664368867874146\n",
      "found similar document with id 5689 in position 0 with similarity score 0.7234717011451721\n",
      "mean percentiles: 95.00\n",
      "========== Saving best model with mean_percentiles: 95.00 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 09:52:59,347 : INFO : saved model/alodokter-articles-doc2vec.model\n",
      "2020-02-23 09:52:59,348 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-02-23 09:52:59,349 : INFO : training model with 5 workers on 15409 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 09:53:00,352 : INFO : EPOCH 1 - PROGRESS: at 37.10% examples, 1530072 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 09:53:01,365 : INFO : EPOCH 1 - PROGRESS: at 74.47% examples, 1521657 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 09:53:02,311 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-02-23 09:53:02,313 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 09:53:02,316 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 09:53:02,317 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 09:53:02,319 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 09:53:02,319 : INFO : EPOCH - 1 : training on 4866436 raw words (4113077 effective words) took 3.0s, 1385887 effective words/s\n",
      "2020-02-23 09:53:02,320 : INFO : training on a 4866436 raw words (4113077 effective words) took 3.0s, 1384554 effective words/s\n",
      "2020-02-23 09:53:02,349 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-02-23 09:53:02,349 : INFO : training model with 5 workers on 15409 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found similar document with id 3893 in position 0 with similarity score 0.7209415435791016\n",
      "found similar document with id 4659 in position 0 with similarity score 0.5547820329666138\n",
      "found similar document with id 5367 in position 0 with similarity score 0.5608341693878174\n",
      "found similar document with id 5493 in position 0 with similarity score 0.5272362232208252\n",
      "found similar document with id 6636 in position 0 with similarity score 0.6744707226753235\n",
      "found similar document with id 2982 in position 0 with similarity score 0.6794629096984863\n",
      "found similar document with id 6973 in position 0 with similarity score 0.5008974671363831\n",
      "found similar document with id 6066 in position 0 with similarity score 0.693284809589386\n",
      "found similar document with id 5500 in position 0 with similarity score 0.672776460647583\n",
      "mean percentiles: 90.00\n",
      "current mean_percentiles: 90.00, best: 95.00\n",
      "training epoch 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 09:53:03,369 : INFO : EPOCH 1 - PROGRESS: at 22.39% examples, 898993 words/s, in_qsize 10, out_qsize 0\n",
      "2020-02-23 09:53:04,371 : INFO : EPOCH 1 - PROGRESS: at 56.69% examples, 1158463 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 09:53:05,375 : INFO : EPOCH 1 - PROGRESS: at 95.01% examples, 1294459 words/s, in_qsize 10, out_qsize 0\n",
      "2020-02-23 09:53:05,495 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-02-23 09:53:05,506 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 09:53:05,511 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 09:53:05,514 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 09:53:05,515 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 09:53:05,515 : INFO : EPOCH - 1 : training on 4866436 raw words (4113602 effective words) took 3.2s, 1300716 effective words/s\n",
      "2020-02-23 09:53:05,516 : INFO : training on a 4866436 raw words (4113602 effective words) took 3.2s, 1299322 effective words/s\n",
      "2020-02-23 09:53:05,547 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-02-23 09:53:05,548 : INFO : training model with 5 workers on 15409 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found similar document with id 3081 in position 0 with similarity score 0.6022744178771973\n",
      "found similar document with id 1444 in position 0 with similarity score 0.5619080662727356\n",
      "found similar document with id 7296 in position 2 with similarity score 0.5168426036834717\n",
      "found similar document with id 3862 in position 0 with similarity score 0.6275359392166138\n",
      "found similar document with id 5078 in position 0 with similarity score 0.6552468538284302\n",
      "found similar document with id 4145 in position 0 with similarity score 0.7030065655708313\n",
      "found similar document with id 1332 in position 1 with similarity score 0.4750661253929138\n",
      "found similar document with id 5062 in position 0 with similarity score 0.6510938405990601\n",
      "mean percentiles: 77.00\n",
      "current mean_percentiles: 77.00, best: 95.00\n",
      "training epoch 3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 09:53:06,552 : INFO : EPOCH 1 - PROGRESS: at 36.17% examples, 1485612 words/s, in_qsize 10, out_qsize 0\n",
      "2020-02-23 09:53:07,558 : INFO : EPOCH 1 - PROGRESS: at 64.79% examples, 1332341 words/s, in_qsize 10, out_qsize 0\n",
      "2020-02-23 09:53:08,560 : INFO : EPOCH 1 - PROGRESS: at 99.03% examples, 1354957 words/s, in_qsize 5, out_qsize 0\n",
      "2020-02-23 09:53:08,566 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-02-23 09:53:08,567 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 09:53:08,574 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 09:53:08,575 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 09:53:08,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 09:53:08,585 : INFO : EPOCH - 1 : training on 4866436 raw words (4113119 effective words) took 3.0s, 1356201 effective words/s\n",
      "2020-02-23 09:53:08,586 : INFO : training on a 4866436 raw words (4113119 effective words) took 3.0s, 1354345 effective words/s\n",
      "2020-02-23 09:53:08,622 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-02-23 09:53:08,623 : INFO : training model with 5 workers on 15409 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found similar document with id 7032 in position 0 with similarity score 0.5312426686286926\n",
      "found similar document with id 7145 in position 0 with similarity score 0.5309252738952637\n",
      "found similar document with id 871 in position 0 with similarity score 0.5175334811210632\n",
      "found similar document with id 5977 in position 2 with similarity score 0.5449908375740051\n",
      "found similar document with id 704 in position 7 with similarity score 0.5392845869064331\n",
      "found similar document with id 7649 in position 0 with similarity score 0.5665024518966675\n",
      "found similar document with id 2926 in position 0 with similarity score 0.5165708661079407\n",
      "found similar document with id 3268 in position 0 with similarity score 0.6013530492782593\n",
      "found similar document with id 6692 in position 0 with similarity score 0.6598948240280151\n",
      "mean percentiles: 81.00\n",
      "current mean_percentiles: 81.00, best: 95.00\n",
      "training epoch 4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 09:53:09,632 : INFO : EPOCH 1 - PROGRESS: at 33.55% examples, 1372494 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 09:53:10,641 : INFO : EPOCH 1 - PROGRESS: at 70.13% examples, 1434771 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 09:53:11,438 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-02-23 09:53:11,440 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 09:53:11,442 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 09:53:11,443 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 09:53:11,452 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 09:53:11,453 : INFO : EPOCH - 1 : training on 4866436 raw words (4112814 effective words) took 2.8s, 1455009 effective words/s\n",
      "2020-02-23 09:53:11,453 : INFO : training on a 4866436 raw words (4112814 effective words) took 2.8s, 1453446 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found similar document with id 5085 in position 0 with similarity score 0.5799938440322876\n",
      "found similar document with id 7348 in position 0 with similarity score 0.5145840644836426\n",
      "found similar document with id 6292 in position 0 with similarity score 0.5893575549125671\n",
      "found similar document with id 2687 in position 0 with similarity score 0.5952913761138916\n",
      "found similar document with id 5110 in position 0 with similarity score 0.6060895919799805\n",
      "found similar document with id 7198 in position 0 with similarity score 0.618311882019043\n",
      "found similar document with id 6849 in position 0 with similarity score 0.5817482471466064\n",
      "found similar document with id 7487 in position 0 with similarity score 0.5786994695663452\n",
      "mean percentiles: 80.00\n",
      "current mean_percentiles: 80.00, best: 95.00\n",
      "early stop...\n",
      "========== Saving best model with mean_percentiles: 95.00 ==========\n"
     ]
    }
   ],
   "source": [
    "model = train(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
