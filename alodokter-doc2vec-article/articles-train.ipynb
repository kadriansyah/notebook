{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install numpy pandas matplotlib sklearn seaborn\n",
    "# !{sys.executable} -m pip install --upgrade gensim\n",
    "# !{sys.executable} -m pip install nltk\n",
    "# !{sys.executable} -m pip install beautifulsoup4\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import multiprocessing\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_FILES = \"\"\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\"\"\"\n",
    "read training files\n",
    "\"\"\"\n",
    "def read_files(path):\n",
    "    print(\"path: {}...\".format(path))\n",
    "    for root, dirnames, filenames in os.walk(path):\n",
    "        for dirname in dirnames:\n",
    "            read_files(os.path.join(root, dirname))\n",
    "        for filename in filenames:\n",
    "            if filename not in SKIP_FILES:\n",
    "                filepath = os.path.join(root, filename)\n",
    "                if os.path.isfile(filepath):\n",
    "                    lines = []\n",
    "                    f = open(filepath, encoding='latin-1')\n",
    "                    for line in f:\n",
    "                        lines.append(line)\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield filename, content\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\"Download a file if not present\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"downloading {}...\".format(filename))\n",
    "        filename, _ = request.urlretrieve(url + filename, filename)\n",
    "\n",
    "        print(\"extracting {}...\".format(filename))\n",
    "        with zipfile.ZipFile(filename) as f:\n",
    "            f.extractall()\n",
    "\n",
    "    \"\"\"directory model for saving model while training\"\"\"\n",
    "    if not os.path.exists('model'):\n",
    "        os.mkdir('model')\n",
    "        print(\"directory model created...\")\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, filename):\n",
    "    filename = download(url, filename)\n",
    "    data_path = filename.replace('.zip','')\n",
    "    documents = []\n",
    "    print(\"building documents...\")\n",
    "    for fname, text in read_files(data_path):\n",
    "        documents.append(clean_str(text).split(' '))\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]\n",
    "    print(\"building documents done\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading data.zip...\n"
     ]
    }
   ],
   "source": [
    "# download data training\n",
    "documents = get_data(url='https://github.com/kadriansyah/notebook/raw/master/alodokter-doc2vec-article/', filename=\"data.zip\")\n",
    "print(\"we have {} documents\".format(len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['penyebab', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'dan', 'cara', 'mengatasinya', 'bayi', 'muntah', 'setelah', 'minum', 'asi', '(', 'air', 'susu', 'ibu', ')', 'adalah', 'keluhan', 'yang', 'sering', 'terjadi', 'sebagian', 'bayi', 'bahkan', 'mengalaminya', 'hampir', 'setiap', 'kali', 'selesai', 'menyusu', 'meski', 'umumnya', 'normal', 'kondisi', 'ini', 'bisa', 'juga', 'disebabkan', 'oleh', 'gangguan', 'berbahaya', 'yang', 'harus', 'diwaspadai', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'dikenal', 'dengan', 'istilah', 'gumoh', 'gumoh', 'dikatakan', 'normal', 'apabila', 'tidak', 'menyebabkan', 'bayi', 'rewel', 'atau', 'sesak', 'napas', 'meskipun', 'dapat', 'dicegah', 'kondisi', 'tersebut', 'tidak', 'memerlukan', 'penanganan', 'khusus', 'dan', 'normal', 'terjadi', 'penyebab', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'gumoh', 'disebabkan', 'oleh', 'asi', 'atau', 'susu', 'yang', 'ditelan', 'bayi', 'kembali', 'ke', 'kerongkongan', 'karena', 'otot', 'di', 'saluran', 'pencernaan', 'bayi', 'yaitu', 'di', 'bagian', 'kerongkongan', 'dan', 'lambung', 'masih', 'lemah', 'kondisi', 'ini', 'disebut', 'sebagai', 'refluks', 'bayi', 'kemungkinan', 'mengalami', 'refluks', 'karena', 'ukuran', 'lambungnya', 'masih', 'sangat', 'kecil', 'sehingga', 'cepat', 'terisi', 'penuh', 'refluks', 'juga', 'terjadi', 'karena', 'katup', 'pada', 'kerongkongan', 'belum', 'sempurna', 'sehingga', 'belum', 'bekerja', 'secara', 'optimal', 'untuk', 'menahan', 'isi', 'lambung', 'umumnya', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'akan', 'berlangsung', 'hingga', 'usia', '4', '5', 'bulan', 'setelah', 'itu', 'gumoh', 'akan', 'berhenti', 'dengan', 'sendirinya', 'penyebab', 'lain', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'adalah', 'gastroenteritis', 'hanya', 'saja', 'infeksi', 'pada', 'saluran', 'cerna', 'bayi', 'ini', 'biasanya', 'disertai', 'dengan', 'diare', 'selain', 'gastoenteritis', 'ada', 'berbagai', 'penyebab', 'lain', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'mulai', 'dari', 'alergi', 'pilek', 'infeksi', 'telinga', 'infeksi', 'saluran', 'kemih', 'hingga', 'penyempitan', 'lambung', '(', 'stenosis', 'pilorus', ')', 'walaupun', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'sering', 'kali', 'disebabkan', 'oleh', 'gumoh', 'yang', 'normal', 'tapi', 'orang', 'tua', 'harus', 'tetap', 'mewaspadai', 'jika', 'bayi', 'muntah', 'disertai', 'dengan', 'gejala', 'lain', 'seperti', 'demam', 'kurang', 'mau', 'atau', 'tidak', 'mau', 'menyusu', 'sama', 'sekali', 'timbul', 'ruam', 'sulit', 'tidur', 'dan', 'rewel', 'ubun', 'ubun', 'tampak', 'menonjol', 'perut', 'bengkak', 'sesak', 'napas', 'muntah', 'disertai', 'darah', 'atau', 'cairan', 'hijau', 'muntah', 'terus', 'menerus', 'lebih', 'lebih', 'dari', 'satu', 'atau', 'dua', 'hari', 'mengalami', 'dehidrasi', 'yang', 'ditandai', 'dengan', 'bibir', 'kering', 'menangis', 'tanpa', 'air', 'mata', 'ubun', 'ubun', 'cekung', 'dan', 'jarang', 'buang', 'air', 'kecil', 'tips', 'meringankan', 'muntah', 'pada', 'bayi', 'bayi', 'gumoh', 'biasanya', 'tidak', 'perlu', 'dikhawatirkan', 'dan', 'akan', 'mereda', 'dengan', 'sendirinya', 'seiring', 'bertambahnya', 'usia', 'bayi', 'meski', 'demikian', 'ada', 'beberapa', 'cara', 'yang', 'dapat', 'dilakukan', 'untuk', 'meringankan', 'keluhan', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'upayakan', 'posisi', 'kepala', 'bayi', 'lebih', 'tinggi', 'dari', 'tubuhnya', 'saat', 'menyusu', 'posisikan', 'tubuhnya', 'tetap', 'tegak', 'setelah', 'menyusu', 'agar', 'bayi', 'dapat', 'lebih', 'mudah', 'bersendawa', 'biarkan', 'bayi', 'menyusu', 'dalam', 'keadaan', 'tenang', 'hal', 'ini', 'akan', 'mencegah', 'bayi', 'mengisap', 'terlalu', 'banyak', 'udara', 'bersamaan', 'dengan', 'asi', 'biasakan', 'bayi', 'menyusu', 'secukupnya', 'namun', 'lebih', 'sering', 'menyusu', 'terlalu', 'banyak', 'dapat', 'membuat', 'lambung', 'bayi', 'teregang', 'karena', 'penuh', 'sehingga', 'memicu', 'bayi', 'untuk', 'muntah', 'setelah', 'minum', 'asi', 'buat', 'bayi', 'sendawa', 'setiap', 'kali', 'habis', 'menyusu', 'biarkan', 'bayi', 'sendawa', 'terlebih', 'dulu', 'sebelum', 'berganti', 'payudara', 'pastikan', 'pakaian', 'atau', 'popok', 'bayi', 'tidak', 'terlalu', 'ketat', 'serta', 'hindari', 'menggendong', 'bayi', 'untuk', 'sendawa', 'dengan', 'posisi', 'perut', 'bayi', 'tepat', 'di', 'bahu', 'anda', 'hal', 'ini', 'untuk', 'mengurangi', 'tekanan', 'pada', 'perutnya', 'hindari', 'menggoyangkan', 'bayi', 'atau', 'membuat', 'bayi', 'aktif', 'segera', 'setelah', 'menyusu', 'sebaiknya', 'juga', 'jangan', 'bepergian', 'dengan', 'kendaraan', 'sesaat', 'setelah', 'bayi', 'menyusu', 'jika', 'bayi', 'sudah', 'cukup', 'besar', 'posisikan', 'agar', 'ia', 'duduk', 'sekitar', '30', 'menit', 'setelah', 'menyusu', 'posisikan', 'kepala', 'bayi', 'sedikit', 'lebih', 'tinggi', 'saat', 'tidur', 'anda', 'dapat', 'meletakkan', 'selimut', 'atau', 'handuk', 'yang', 'digulung', 'di', 'bawah', 'bahu', 'dan', 'kepalanya', 'sebaiknya', 'hindari', 'menggunakan', 'bantal', 'pada', 'bayi', 'teliti', 'kemungkinan', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'akibat', 'makanan', 'atau', 'minuman', 'yang', 'dikonsumsi', 'ibu', 'misalnya', 'susu', 'sapi', 'jika', 'bayi', 'muntah', 'setelah', 'minum', 'asi', 'disertai', 'tanda', 'tanda', 'bahaya', 'di', 'atas', 'atau', 'jika', 'anda', 'merasa', 'khawatir', 'akan', 'kondisi', 'ini', 'segeralah', 'konsultasikan', 'dengan', 'dokter', 'anak', 'catat', 'berapa', 'kali', 'atau', 'berapa', 'banyak', 'bayi', 'muntah', 'dan', 'apakah', 'terdapat', 'gejala', 'gejala', 'lainnya'], tags=[0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, documents, steps):\n",
    "    percentiles = np.zeros(steps)\n",
    "    for step in range(steps):\n",
    "        docid = np.random.randint(model.docvecs.count)\n",
    "        inferred_vector = model.infer_vector(documents[docid][0])\n",
    "        similars = model.docvecs.most_similar(positive=[inferred_vector], topn=10)\n",
    "        for idx,simdoc in enumerate(similars):\n",
    "            if simdoc[0] == docid:\n",
    "                print(\"found similar document with id {} in position {} with similarity score {}\".format(simdoc[0], idx, simdoc[1]))\n",
    "                percentiles[step] = ((len(similars) - idx) / len(similars)) * 100\n",
    "                break\n",
    "    return np.mean(percentiles)\n",
    "\n",
    "def train(documents=documents, model_name=\"model/alodokter-articles-doc2vec.model\", max_epochs=50, patience=3):\n",
    "    best_mean_percentiles = 0\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = Doc2Vec(dm=1, vector_size=300, window=2, alpha=0.1, min_alpha=0.0001, min_count=5, epochs=1, workers=5)\n",
    "    model.build_vocab(documents)\n",
    "    for epoch in range(max_epochs):\n",
    "        print('training epoch {:d} ...'.format(epoch))\n",
    "        model.train(documents, total_examples=model.corpus_count,epochs=model.epochs)\n",
    "        mean_percentiles = evaluate(model,documents,10)\n",
    "        print('mean percentiles: {:.2f}'.format(mean_percentiles))\n",
    "        \n",
    "        if mean_percentiles < best_mean_percentiles:\n",
    "            print(\"current mean_percentiles: {:.2f}, best: {:.2f}\".format(mean_percentiles, best_mean_percentiles))\n",
    "            patience = patience-1\n",
    "        else:\n",
    "            best_mean_percentiles = mean_percentiles\n",
    "            print(\"========== Saving best model with mean_percentiles: {:.2f} ==========\".format(mean_percentiles))\n",
    "            model.save(model_name)\n",
    "            patience = patience+1\n",
    "        \n",
    "        if patience == 0:\n",
    "            print(\"early stop...\")\n",
    "            print(\"========== Saving best model with mean_percentiles: {:.2f} ==========\".format(best_mean_percentiles))\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 08:46:41,928 : INFO : collecting all words and their counts\n",
      "2020-02-23 08:46:41,928 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-02-23 08:46:42,580 : INFO : collected 54008 word types and 7932 unique tags from a corpus of 7932 examples and 4866436 words\n",
      "2020-02-23 08:46:42,581 : INFO : Loading a fresh vocabulary\n",
      "2020-02-23 08:46:42,613 : INFO : effective_min_count=5 retains 15409 unique words (28% of original 54008, drops 38599)\n",
      "2020-02-23 08:46:42,614 : INFO : effective_min_count=5 leaves 4811064 word corpus (98% of original 4866436, drops 55372)\n",
      "2020-02-23 08:46:42,654 : INFO : deleting the raw counts dictionary of 54008 items\n",
      "2020-02-23 08:46:42,656 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2020-02-23 08:46:42,657 : INFO : downsampling leaves estimated 4105293 word corpus (85.3% of prior 4811064)\n",
      "2020-02-23 08:46:42,698 : INFO : estimated required memory for 15409 words and 300 dimensions: 54204500 bytes\n",
      "2020-02-23 08:46:42,699 : INFO : resetting layer weights\n",
      "2020-02-23 08:46:46,362 : INFO : training model with 5 workers on 15409 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 08:46:47,375 : INFO : EPOCH 1 - PROGRESS: at 36.91% examples, 1508902 words/s, in_qsize 10, out_qsize 0\n",
      "2020-02-23 08:46:48,379 : INFO : EPOCH 1 - PROGRESS: at 75.47% examples, 1541982 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 08:46:49,022 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-02-23 08:46:49,024 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 08:46:49,025 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 08:46:49,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 08:46:49,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 08:46:49,032 : INFO : EPOCH - 1 : training on 4866436 raw words (4113442 effective words) took 2.7s, 1542559 effective words/s\n",
      "2020-02-23 08:46:49,033 : INFO : training on a 4866436 raw words (4113442 effective words) took 2.7s, 1540515 effective words/s\n",
      "2020-02-23 08:46:49,034 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-02-23 08:46:49,082 : INFO : saving Doc2Vec object under model/alodokter-articles-doc2vec.model, separately None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found similar document with id 3538 in position 0 with similarity score 0.6707550883293152\n",
      "found similar document with id 5152 in position 0 with similarity score 0.7327303290367126\n",
      "found similar document with id 7281 in position 0 with similarity score 0.7627062201499939\n",
      "found similar document with id 4481 in position 0 with similarity score 0.7637534141540527\n",
      "found similar document with id 2770 in position 0 with similarity score 0.7537136077880859\n",
      "found similar document with id 6782 in position 0 with similarity score 0.7837415337562561\n",
      "found similar document with id 2088 in position 0 with similarity score 0.613426923751831\n",
      "found similar document with id 5165 in position 0 with similarity score 0.7269462943077087\n",
      "found similar document with id 5539 in position 0 with similarity score 0.7787278890609741\n",
      "found similar document with id 3822 in position 0 with similarity score 0.5798944234848022\n",
      "mean percentiles: 100.00\n",
      "========== Saving best model with mean_percentiles: 100.00 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 08:46:49,536 : INFO : saved model/alodokter-articles-doc2vec.model\n",
      "2020-02-23 08:46:49,537 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-02-23 08:46:49,537 : INFO : training model with 5 workers on 15409 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 08:46:50,549 : INFO : EPOCH 1 - PROGRESS: at 31.42% examples, 1279618 words/s, in_qsize 10, out_qsize 0\n",
      "2020-02-23 08:46:51,561 : INFO : EPOCH 1 - PROGRESS: at 66.75% examples, 1362452 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 08:46:52,409 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-02-23 08:46:52,410 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 08:46:52,411 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 08:46:52,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 08:46:52,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 08:46:52,415 : INFO : EPOCH - 1 : training on 4866436 raw words (4113779 effective words) took 2.9s, 1430908 effective words/s\n",
      "2020-02-23 08:46:52,416 : INFO : training on a 4866436 raw words (4113779 effective words) took 2.9s, 1429390 effective words/s\n",
      "2020-02-23 08:46:52,446 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-02-23 08:46:52,447 : INFO : training model with 5 workers on 15409 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found similar document with id 1532 in position 0 with similarity score 0.6240188479423523\n",
      "found similar document with id 4786 in position 0 with similarity score 0.6457080841064453\n",
      "found similar document with id 4638 in position 0 with similarity score 0.6878639459609985\n",
      "found similar document with id 431 in position 3 with similarity score 0.5665192604064941\n",
      "found similar document with id 2177 in position 0 with similarity score 0.6415995359420776\n",
      "found similar document with id 2228 in position 1 with similarity score 0.6698856353759766\n",
      "found similar document with id 6718 in position 0 with similarity score 0.7500876784324646\n",
      "found similar document with id 3492 in position 0 with similarity score 0.667715311050415\n",
      "found similar document with id 4413 in position 0 with similarity score 0.721293568611145\n",
      "found similar document with id 1888 in position 0 with similarity score 0.696130096912384\n",
      "mean percentiles: 96.00\n",
      "current mean_percentiles: 96.00, best: 100.00\n",
      "training epoch 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 08:46:53,451 : INFO : EPOCH 1 - PROGRESS: at 32.63% examples, 1339976 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 08:46:54,457 : INFO : EPOCH 1 - PROGRESS: at 68.76% examples, 1412554 words/s, in_qsize 9, out_qsize 1\n",
      "2020-02-23 08:46:55,259 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-02-23 08:46:55,261 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 08:46:55,262 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 08:46:55,263 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 08:46:55,267 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 08:46:55,268 : INFO : EPOCH - 1 : training on 4866436 raw words (4113356 effective words) took 2.8s, 1460006 effective words/s\n",
      "2020-02-23 08:46:55,268 : INFO : training on a 4866436 raw words (4113356 effective words) took 2.8s, 1458161 effective words/s\n",
      "2020-02-23 08:46:55,299 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-02-23 08:46:55,300 : INFO : training model with 5 workers on 15409 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found similar document with id 1782 in position 0 with similarity score 0.6951780319213867\n",
      "found similar document with id 3722 in position 0 with similarity score 0.5354634523391724\n",
      "found similar document with id 4457 in position 0 with similarity score 0.6111356616020203\n",
      "found similar document with id 1609 in position 0 with similarity score 0.5046414136886597\n",
      "found similar document with id 6076 in position 0 with similarity score 0.6949859857559204\n",
      "found similar document with id 7870 in position 0 with similarity score 0.5444523692131042\n",
      "found similar document with id 4978 in position 0 with similarity score 0.668070912361145\n",
      "found similar document with id 4951 in position 0 with similarity score 0.708315372467041\n",
      "found similar document with id 3603 in position 0 with similarity score 0.6080686450004578\n",
      "mean percentiles: 90.00\n",
      "current mean_percentiles: 90.00, best: 100.00\n",
      "training epoch 3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 08:46:56,313 : INFO : EPOCH 1 - PROGRESS: at 34.30% examples, 1401116 words/s, in_qsize 10, out_qsize 0\n",
      "2020-02-23 08:46:57,314 : INFO : EPOCH 1 - PROGRESS: at 70.13% examples, 1439043 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 08:46:58,076 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-02-23 08:46:58,078 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 08:46:58,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 08:46:58,081 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 08:46:58,083 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 08:46:58,084 : INFO : EPOCH - 1 : training on 4866436 raw words (4113458 effective words) took 2.8s, 1479933 effective words/s\n",
      "2020-02-23 08:46:58,084 : INFO : training on a 4866436 raw words (4113458 effective words) took 2.8s, 1477936 effective words/s\n",
      "2020-02-23 08:46:58,111 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-02-23 08:46:58,112 : INFO : training model with 5 workers on 15409 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found similar document with id 2021 in position 0 with similarity score 0.5519502758979797\n",
      "found similar document with id 7898 in position 0 with similarity score 0.6792017221450806\n",
      "found similar document with id 5719 in position 0 with similarity score 0.5279531478881836\n",
      "found similar document with id 1691 in position 1 with similarity score 0.5611472129821777\n",
      "found similar document with id 1470 in position 0 with similarity score 0.4522853493690491\n",
      "found similar document with id 7235 in position 0 with similarity score 0.585452675819397\n",
      "found similar document with id 1369 in position 0 with similarity score 0.5770550966262817\n",
      "found similar document with id 5705 in position 0 with similarity score 0.664800763130188\n",
      "found similar document with id 7664 in position 0 with similarity score 0.5632209777832031\n",
      "found similar document with id 3482 in position 1 with similarity score 0.5628969669342041\n",
      "mean percentiles: 98.00\n",
      "current mean_percentiles: 98.00, best: 100.00\n",
      "training epoch 4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 08:46:59,118 : INFO : EPOCH 1 - PROGRESS: at 36.17% examples, 1482893 words/s, in_qsize 9, out_qsize 0\n",
      "2020-02-23 08:47:00,118 : INFO : EPOCH 1 - PROGRESS: at 74.04% examples, 1522234 words/s, in_qsize 10, out_qsize 0\n",
      "2020-02-23 08:47:00,809 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-02-23 08:47:00,812 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 08:47:00,813 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 08:47:00,816 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 08:47:00,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 08:47:00,822 : INFO : EPOCH - 1 : training on 4866436 raw words (4114260 effective words) took 2.7s, 1519993 effective words/s\n",
      "2020-02-23 08:47:00,823 : INFO : training on a 4866436 raw words (4114260 effective words) took 2.7s, 1517779 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found similar document with id 5933 in position 0 with similarity score 0.4468348026275635\n",
      "found similar document with id 2150 in position 0 with similarity score 0.5843720436096191\n",
      "found similar document with id 3330 in position 0 with similarity score 0.6252533197402954\n",
      "found similar document with id 6725 in position 0 with similarity score 0.6015385389328003\n",
      "found similar document with id 7817 in position 1 with similarity score 0.5528010129928589\n",
      "found similar document with id 5162 in position 0 with similarity score 0.6628072261810303\n",
      "found similar document with id 5580 in position 0 with similarity score 0.5757412314414978\n",
      "found similar document with id 1551 in position 2 with similarity score 0.5404307842254639\n",
      "found similar document with id 3883 in position 0 with similarity score 0.5876838564872742\n",
      "found similar document with id 5585 in position 0 with similarity score 0.5757378339767456\n",
      "mean percentiles: 97.00\n",
      "current mean_percentiles: 97.00, best: 100.00\n",
      "early stop...\n",
      "========== Saving best model with mean_percentiles: 100.00 ==========\n"
     ]
    }
   ],
   "source": [
    "model = train(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
