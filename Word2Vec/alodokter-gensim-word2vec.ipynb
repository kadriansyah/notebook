{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### source: https://github.com/adventuresinML/adventures-in-ml-code/blob/master/gensim_word2vec.py\n",
    "##### source: https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word\n",
    "##### source: https://radimrehurek.com/gensim/models/phrases.html\n",
    "##### source: http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install numpy pandas matplotlib sklearn seaborn\n",
    "# !{sys.executable} -m pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.preprocessing  import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Reshape, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "vector_dim = 300\n",
    "\n",
    "def build_phrases(sentences, model_name='phrases.model'):\n",
    "    phrases = Phrases(sentences, min_count=5, threshold=7, progress_per=1000)\n",
    "    phrases_model = Phraser(phrases)\n",
    "    phrases_model.save(model_name)\n",
    "    return phrases_model\n",
    "\n",
    "def sentences_to_bigrams(phrases_model, sentences):\n",
    "    bigrams_sentences = []\n",
    "    for sentence in sentences:\n",
    "        phrases_sentence = phrases_model[sentence]\n",
    "        bigrams_sentences.append(phrases_sentence)\n",
    "    return bigrams_sentences\n",
    "\n",
    "def get_data(filename=\"questions.dat\"):\n",
    "    sentences = []\n",
    "    dataset = tf.data.TextLineDataset(filename)\n",
    "    dataset = dataset.enumerate() \n",
    "    for element in dataset.as_numpy_iterator():\n",
    "        text = element[1].decode(\"utf-8\")\n",
    "        sentences.append(text.split(' '))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name=\"alodokter-word2vec.model\"):\n",
    "    sentences = get_data()\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = word2vec.Word2Vec(sentences, iter=10, min_count=10, size=300, workers=4)\n",
    "    model.save(model_name)\n",
    "    return model\n",
    "\n",
    "def train_bigrams(model_name=\"alodokter-word2vec-bigram.model\"):\n",
    "    sentences = get_data()\n",
    "    phrases_model = build_phrases(sentences)\n",
    "    sentences = sentences_to_bigrams(phrases_model, sentences)\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = word2vec.Word2Vec(sentences, iter=10, min_count=10, size=300, workers=4)\n",
    "    model.save(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(model):\n",
    "    # convert the wv word vectors into a numpy matrix that is suitable for insertion into our TensorFlow and Keras models\n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), vector_dim))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def keras_model(embedding_matrix, wv):\n",
    "    valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "    \n",
    "    # input words - in this case we do sample by sample evaluations of the similarity\n",
    "    valid_word = Input((1,), dtype='int32')\n",
    "    other_word = Input((1,), dtype='int32')\n",
    "    \n",
    "    # setup the embedding layer\n",
    "    embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix])\n",
    "    embedded_a = embeddings(valid_word)\n",
    "    embedded_b = embeddings(other_word)\n",
    "    similarity = Dot(name=\"Cosine-Similarity\", axes=2, normalize=True)([embedded_a, embedded_b])\n",
    "\n",
    "    # create the Keras model\n",
    "    k_model = Model(inputs=[valid_word, other_word], outputs=similarity)\n",
    "\n",
    "    def get_similarity(valid_word_idx, vocab_size):\n",
    "        similarities = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = k_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            similarities[i] = out\n",
    "        return similarities\n",
    "\n",
    "    # now run the model and get the closest words to the valid examples\n",
    "    for i in range(valid_size):\n",
    "        valid_word = wv.index2word[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        similarity = get_similarity(valid_examples[i], len(wv.vocab))\n",
    "        nearest = (-similarity).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "            close_word = wv.index2word[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "\n",
    "def tensorflow_model(embedding_matrix, wv):\n",
    "    valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # embedding layer weights are frozen to avoid updating embeddings while training\n",
    "    saved_embeddings = tf.constant(embedding_matrix)\n",
    "    embedding = tf.Variable(initial_value=saved_embeddings, trainable=False)\n",
    "\n",
    "    # create the cosine similarity operations\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
    "    normalized_embeddings = embedding / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # call our similarity operation\n",
    "    sim = similarity.numpy()\n",
    "    \n",
    "    # run through each valid example, finding closest words\n",
    "    for i in range(valid_size):\n",
    "        valid_word = wv.index2word[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "            close_word = wv.index2word[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test Keras Model \"\"\"\n",
    "training = False\n",
    "if training:\n",
    "    model = train()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"alodokter-word2vec.model\")\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(model)\n",
    "keras_model(embedding_matrix, model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test Keras Model Bigrams \"\"\"\n",
    "training = False\n",
    "if training:\n",
    "    model = train()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"alodokter-word2vec-bigram.model\")\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(model)\n",
    "keras_model(embedding_matrix, model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test Tensorflow Model \"\"\"\n",
    "training = False\n",
    "if training:\n",
    "    model = train()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"alodokter-word2vec.model\")\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(model)\n",
    "tensorflow_model(embedding_matrix, model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test Tensorflow Model Bigrams \"\"\"\n",
    "training = False\n",
    "if training:\n",
    "    model = train()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"alodokter-word2vec-bigram.model\")\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(model)\n",
    "tensorflow_model(embedding_matrix, model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = False\n",
    "if training:\n",
    "    model = train()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"alodokter-word2vec.model\")\n",
    "model.wv.most_similar(positive=['kemaluan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = False\n",
    "if training:\n",
    "    model = train_bigrams()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"alodokter-word2vec-bigram.model\")\n",
    "model.wv.most_similar(positive=['kemaluan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
