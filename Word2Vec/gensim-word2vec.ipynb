{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### source: https://github.com/adventuresinML/adventures-in-ml-code/blob/master/gensim_word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install numpy pandas matplotlib sklearn seaborn\n",
    "# !{sys.executable} -m pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.preprocessing  import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Reshape, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "vector_dim = 300\n",
    "\n",
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = f.read(f.namelist()[0]).split()\n",
    "    return data\n",
    "\n",
    "def convert_data_to_index(string_data, wv):\n",
    "    index_data = []\n",
    "    for word in string_data:\n",
    "        if word in wv:\n",
    "            index_data.append(wv.vocab[word].index)\n",
    "    return index_data\n",
    "\n",
    "def collect_data():\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = maybe_download('text8.zip', url, 31344016)\n",
    "    if not os.path.exists(filename.strip('.zip')):\n",
    "        zipfile.ZipFile(filename).extractall()\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name=\"word2vec-gensim.model\"):\n",
    "    filename = collect_data()\n",
    "    \n",
    "    sentences = word2vec.Text8Corpus(filename.strip('.zip'))\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = word2vec.Word2Vec(sentences, iter=10, min_count=10, size=300, workers=4)\n",
    "\n",
    "    str_data = read_data(filename)\n",
    "    index_data = convert_data_to_index(str_data, model.wv)\n",
    "    print(str_data[:4], index_data[:4])\n",
    "    \n",
    "    model.save(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(model):\n",
    "    # convert the wv word vectors into a numpy matrix that is suitable for insertion\n",
    "    # into our TensorFlow and Keras models\n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), vector_dim))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def keras_model(embedding_matrix, wv):\n",
    "    valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "    \n",
    "    # input words - in this case we do sample by sample evaluations of the similarity\n",
    "    valid_word = Input((1,), dtype='int32')\n",
    "    other_word = Input((1,), dtype='int32')\n",
    "    \n",
    "    # setup the embedding layer\n",
    "    embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix])\n",
    "    embedded_a = embeddings(valid_word)\n",
    "    embedded_b = embeddings(other_word)\n",
    "    similarity = Dot(name=\"Cosine-Similarity\", axes=2, normalize=True)([embedded_a, embedded_b])\n",
    "\n",
    "    # create the Keras model\n",
    "    k_model = Model(inputs=[valid_word, other_word], outputs=similarity)\n",
    "\n",
    "    def get_similarity(valid_word_idx, vocab_size):\n",
    "        similarities = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = k_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            similarities[i] = out\n",
    "        return similarities\n",
    "\n",
    "    # now run the model and get the closest words to the valid examples\n",
    "    for i in range(valid_size):\n",
    "        valid_word = wv.index2word[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        similarity = get_similarity(valid_examples[i], len(wv.vocab))\n",
    "        nearest = (-similarity).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "            close_word = wv.index2word[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "\n",
    "def tensorflow_model(embedding_matrix, wv):\n",
    "    valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # embedding layer weights are frozen to avoid updating embeddings while training\n",
    "    saved_embeddings = tf.constant(embedding_matrix)\n",
    "    embedding = tf.Variable(initial_value=saved_embeddings, trainable=False)\n",
    "\n",
    "    # create the cosine similarity operations\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
    "    normalized_embeddings = embedding / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # call our similarity operation\n",
    "    sim = similarity.numpy()\n",
    "    \n",
    "    # run through each valid example, finding closest words\n",
    "    for i in range(valid_size):\n",
    "        valid_word = wv.index2word[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "            close_word = wv.index2word[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to first: second, last, youngest, fourth, fifth, earliest, sixth, next,\n",
      "Nearest to from: forcibly, therefrom, via, onto, aravalli, across, back, gradually,\n",
      "Nearest to some: many, certain, various, numerous, several, those, few, these,\n",
      "Nearest to no: nothing, none, little, neither, any, hardly, nobody, whatever,\n",
      "Nearest to d: b, politician, theodore, sar, malan, harpsichordist, swimmer, playwright,\n",
      "Nearest to system: systems, filesystem, scheme, network, framework, mechanism, program, apparatus,\n",
      "Nearest to nine: eight, seven, six, one, four, five, three, october,\n",
      "Nearest to this: it, which, that, the, a, what, itself, whatever,\n",
      "Nearest to while: whilst, although, though, whereas, however, thus, but, nevertheless,\n",
      "Nearest to over: across, nearly, around, approximately, throughout, intervening, ago, during,\n",
      "Nearest to used: employed, applied, utilized, utilised, invoked, uses, preferred, useful,\n",
      "Nearest to th: fifteenth, fourteenth, sixteenth, seventeenth, eleventh, nd, eighteenth, thirteenth,\n",
      "Nearest to five: four, six, three, seven, eight, one, two, zero,\n",
      "Nearest to new: alcuin, bissau, rudy, dorchester, equatorial, newest, newark, newly,\n",
      "Nearest to all: invariably, always, everywhere, certain, everyone, identical, various, exclusively,\n",
      "Nearest to most: fairly, extremely, more, highly, less, particularly, vitally, quite,\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test Keras Model \"\"\"\n",
    "training = False\n",
    "if training:\n",
    "    model = train()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"word2vec-gensim.model\")\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(model)\n",
    "keras_model(embedding_matrix, model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to american: canadian, carolina, gary, australian, cuban, america, dakota, bryan,\n",
      "Nearest to this: it, which, that, the, a, what, itself, whatever,\n",
      "Nearest to people: persons, residents, citizens, individuals, jews, inhabitants, americans, albanians,\n",
      "Nearest to united: federated, reorganisation, isambard, polities, micronesia, axumite, oecs, warring,\n",
      "Nearest to up: ablaze, ups, off, alight, down, aside, contours, indentation,\n",
      "Nearest to s: sixties, his, seventies, vought, janet, stanley, christine, ted,\n",
      "Nearest to would: might, will, could, should, shall, must, did, seemed,\n",
      "Nearest to most: fairly, extremely, more, highly, less, particularly, vitally, quite,\n",
      "Nearest to will: would, must, should, shall, might, could, can, may,\n",
      "Nearest to called: termed, referred, valpara, dubbed, named, known, abbreviated, spelled,\n",
      "Nearest to have: having, has, had, possess, exhibit, ve, contain, imply,\n",
      "Nearest to was: is, were, became, had, fell, remained, came, remains,\n",
      "Nearest to that: which, actually, what, therefore, whatever, if, this, clearly,\n",
      "Nearest to by: privately, divinely, poorly, newly, constitutionally, independently, zeppelin, profoundly,\n",
      "Nearest to in: throughout, during, until, since, around, within, expo, one,\n",
      "Nearest to other: various, fewer, different, these, specific, disparate, certain, multiple,\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test Tensorflow Model \"\"\"\n",
    "training = False\n",
    "if training:\n",
    "    model = train()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"word2vec-gensim.model\")\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(model)\n",
    "tensorflow_model(embedding_matrix, model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test Gensim \"\"\"\n",
    "training = False\n",
    "if training:\n",
    "    model = train()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"word2vec-gensim.model\")\n",
    "\n",
    "# get the word vector of \"the\"\n",
    "print(model.wv['the'])\n",
    "\n",
    "# get the most common words\n",
    "print(model.wv.index2word[0], model.wv.index2word[1], model.wv.index2word[2])\n",
    "\n",
    "# get the least common words\n",
    "vocab_size = len(model.wv.vocab)\n",
    "print(model.wv.index2word[vocab_size - 1], model.wv.index2word[vocab_size - 2], model.wv.index2word[vocab_size - 3])\n",
    "\n",
    "# find the index of the 2nd most common word (\"of\")\n",
    "print('Index of \"of\" is: {}'.format(model.wv.vocab['of'].index))\n",
    "\n",
    "# some similarity fun\n",
    "print(model.wv.similarity('woman', 'man'), model.wv.similarity('man', 'elephant'))\n",
    "\n",
    "# what doesn't fit?\n",
    "print(model.wv.doesnt_match(\"green blue red zebra\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
