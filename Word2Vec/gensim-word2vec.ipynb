{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### source: https://github.com/adventuresinML/adventures-in-ml-code/blob/master/gensim_word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install numpy pandas matplotlib sklearn seaborn\n",
    "# !{sys.executable} -m pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.preprocessing  import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Reshape, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "vector_dim = 300\n",
    "\n",
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = f.read(f.namelist()[0]).split()\n",
    "    return data\n",
    "\n",
    "def convert_data_to_index(string_data, wv):\n",
    "    index_data = []\n",
    "    for word in string_data:\n",
    "        if word in wv:\n",
    "            index_data.append(wv.vocab[word].index)\n",
    "    return index_data\n",
    "\n",
    "def collect_data():\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = maybe_download('text8.zip', url, 31344016)\n",
    "    if not os.path.exists(filename.strip('.zip')):\n",
    "        zipfile.ZipFile(filename).extractall()\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name=\"word2vec-gensim.model\"):\n",
    "    filename = collect_data()\n",
    "    \n",
    "    sentences = word2vec.Text8Corpus(filename.strip('.zip'))\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = word2vec.Word2Vec(sentences, iter=10, min_count=10, size=300, workers=4)\n",
    "\n",
    "    str_data = read_data(filename)\n",
    "    index_data = convert_data_to_index(str_data, model.wv)\n",
    "    print(str_data[:4], index_data[:4])\n",
    "    \n",
    "    model.save(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(model):\n",
    "    # convert the wv word vectors into a numpy matrix that is suitable for insertion\n",
    "    # into our TensorFlow and Keras models\n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), vector_dim))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def keras_model(embedding_matrix, wv):\n",
    "    valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "    \n",
    "    # input words - in this case we do sample by sample evaluations of the similarity\n",
    "    valid_word = Input((1,), dtype='int32')\n",
    "    other_word = Input((1,), dtype='int32')\n",
    "    \n",
    "    # setup the embedding layer\n",
    "    embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix])\n",
    "    embedded_a = embeddings(valid_word)\n",
    "    embedded_b = embeddings(other_word)\n",
    "    similarity = Dot(name=\"Cosine-Similarity\", axes=2, normalize=True)([embedded_a, embedded_b])\n",
    "\n",
    "    # create the Keras model\n",
    "    k_model = Model(inputs=[valid_word, other_word], outputs=similarity)\n",
    "\n",
    "    def get_similarity(valid_word_idx, vocab_size):\n",
    "        similarities = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = k_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            similarities[i] = out\n",
    "        return similarities\n",
    "\n",
    "    # now run the model and get the closest words to the valid examples\n",
    "    for i in range(valid_size):\n",
    "        valid_word = wv.index2word[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        similarity = get_similarity(valid_examples[i], len(wv.vocab))\n",
    "        nearest = (-similarity).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "            close_word = wv.index2word[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to s: sixties, his, seventies, vought, janet, stanley, christine, ted,\n",
      "Nearest to people: persons, residents, citizens, individuals, jews, inhabitants, americans, albanians,\n",
      "Nearest to in: throughout, during, until, since, around, within, expo, one,\n",
      "Nearest to its: their, the, itself, our, it, simplicity, whose, immediate,\n",
      "Nearest to state: legislature, nation, territory, judiciary, government, states, sovereign, statehood,\n",
      "Nearest to be: prevail, proceed, represent, imply, satisfy, suffice, invoke, yield,\n",
      "Nearest to some: many, certain, various, numerous, several, those, few, these,\n",
      "Nearest to was: is, were, became, had, fell, remained, came, remains,\n",
      "Nearest to six: four, eight, five, seven, three, one, nine, two,\n",
      "Nearest to often: sometimes, frequently, usually, generally, typically, commonly, occasionally, normally,\n",
      "Nearest to states: nations, emirates, kingdom, provinces, parcel, methodist, loyalists, wallachia,\n",
      "Nearest to that: which, actually, what, therefore, whatever, if, this, clearly,\n",
      "Nearest to only: just, actually, solely, every, least, still, always, sufficient,\n",
      "Nearest to who: whom, whose, whoever, which, that, personally, deceased, alike,\n",
      "Nearest to time: moment, period, forefront, decade, outset, distance, distances, intervals,\n",
      "Nearest to th: fifteenth, fourteenth, sixteenth, seventeenth, eleventh, nd, eighteenth, thirteenth,\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test Keras Model \"\"\"\n",
    "training = False\n",
    "if training:\n",
    "    model = train()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"word2vec-gensim.model\")\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(model)\n",
    "keras_model(embedding_matrix, model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22422697  0.14712758 -0.47601643  0.3790985   0.10261266 -0.8650235\n",
      " -0.31936124  0.5156032  -0.98320854 -0.789132    0.46489877 -0.82984257\n",
      "  0.09237605 -0.08166566 -0.07527041 -0.85539114  0.8185313   0.27993223\n",
      "  0.5104366   0.18904924  0.5679309   0.5547837   0.3610671   0.73984617\n",
      " -0.5218881   0.11752108 -0.6944164   0.79219466  0.5284484   0.5948236\n",
      " -0.3011668   0.14271732  0.3743451   0.455225    0.77808815 -0.7987459\n",
      " -0.7720395  -0.0787911   1.048541   -0.32106057 -0.21548207  0.20202452\n",
      " -0.44003892 -1.641467   -0.74306124  0.30159307 -0.02842805  0.64166474\n",
      " -1.4520941   0.46092674  0.32494962  0.15518251  0.9517272   0.71376425\n",
      " -0.8646511   0.1489159  -0.5240067   0.19802691  0.5452466   0.32581544\n",
      "  0.2240663  -0.11205453 -0.67169976  1.0156052  -0.8727423  -0.3247499\n",
      " -0.20645174 -0.3241183   0.05699601 -0.59548783  0.5271353   0.94990444\n",
      " -0.06797419 -0.53946453  1.166609   -0.26356938  0.62137777  0.4043672\n",
      " -0.4871116  -0.05922779  0.25737008 -0.5965672  -0.22691219  0.1261594\n",
      "  0.7132027   0.2109369  -0.44850478  1.055543    0.04354802 -0.4254812\n",
      " -0.13394256 -0.3045738  -0.314766   -0.33111975  0.05955091 -0.56637704\n",
      " -0.04673718  0.66345954  0.19378738 -0.17259927  0.0549901   0.01766852\n",
      "  0.02321973  0.614697    0.2523161   0.17866653 -0.5633215   0.8419063\n",
      "  0.1066777   0.18405376  0.44357967 -1.180803    0.59256625 -0.8780137\n",
      "  0.06585514  0.5698286  -0.5193949  -0.37033665  0.11750564  0.18605293\n",
      "  0.7778471   0.21878052 -0.05837634 -0.47325832  1.0429734   0.4512585\n",
      "  0.06545017 -0.47202176  0.2767639   0.91187066 -0.6718831   0.64431256\n",
      "  1.9901568   1.3232294  -0.77927667 -0.18511777 -0.5834753   0.7077617\n",
      " -0.17198202  0.94629264  0.32732144  0.7462822  -0.04870944 -1.129854\n",
      " -0.12862942  0.0231872  -0.44178382  0.45300424  0.10480065  0.9556242\n",
      " -0.0185115   0.18979572  0.03778725  0.27551913 -0.13498087 -0.54726875\n",
      "  0.01797577 -0.03563211 -0.45984876  0.03623415 -0.5645157  -0.07687467\n",
      "  0.54383636 -0.13548799 -1.1333829   0.76053214  0.85719335  0.02861267\n",
      " -0.8428412   0.48232993 -0.24094915  0.05190394  0.13193332  0.26869762\n",
      " -0.10232297 -0.63518417 -0.50865877  0.8897335   0.48869684 -0.02905377\n",
      "  0.1460096  -1.0325437   0.12495042 -0.60034126  0.5557728  -0.6363259\n",
      "  0.36334616  0.01491275 -1.5619551   0.2768179   0.23311105  0.3426897\n",
      "  0.3781936  -0.5084911  -0.3863908   0.10098701  0.08372402 -0.13829656\n",
      "  0.55333656  0.51680917 -0.5281503   1.3680134   0.12986925  0.0302348\n",
      "  0.60252523  0.03808864 -0.6554628   0.6357373  -0.15718654  0.4953122\n",
      " -0.69726676 -0.20283234 -0.79650176  1.3514279  -0.57730347 -0.579811\n",
      "  0.5520781  -0.15977462 -0.21188915 -0.9120686  -0.7625673   0.3115016\n",
      " -0.02544524  0.40267247 -0.40440714 -0.74393415 -0.8454471  -0.09281024\n",
      "  0.26759475 -1.3130246   0.05874567 -0.99847245 -0.04369727  0.16090412\n",
      " -1.4903723  -0.3395161   0.01302611  0.102563   -0.6619519   0.2839446\n",
      " -0.6359678  -0.40072697  0.17553641  0.04610936 -0.7151789   0.08483001\n",
      "  0.22772597  0.16879906 -0.48613644  0.0391636  -0.35244423  0.12565053\n",
      "  0.14082488 -0.2895694   0.01900943  0.18741755 -0.1734542  -0.24034125\n",
      "  0.6604281  -0.5253139  -0.01296615 -0.78890955  0.12884998  0.02847728\n",
      " -0.25069267  0.37277383  0.632294   -0.60858715 -1.1043916  -0.47469035\n",
      "  0.3486993  -0.26794818  0.00229737  0.19065897 -0.9649091   0.5878134\n",
      " -0.50423145  0.26790753 -0.15614459 -0.5196365  -0.5769623  -0.03965413\n",
      "  0.7002426  -1.020656   -0.11897148  0.7632283   0.04152524  0.00583058\n",
      "  0.1982048   0.8249691   1.0648072  -0.1777577   0.6932145   0.08655061\n",
      " -0.4273606  -0.52179265  0.06077643 -0.8717993   0.68179953  0.18751661]\n",
      "the of and\n",
      "kirchenmusik villein meherabad\n",
      "Index of \"of\" is: 1\n",
      "0.5956473 0.1854893\n",
      "zebra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kadriansyah/anaconda3/envs/recommender/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test Gensim \"\"\"\n",
    "training = False\n",
    "if training:\n",
    "    model = train()\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"word2vec-gensim.model\")\n",
    "\n",
    "# get the word vector of \"the\"\n",
    "print(model.wv['the'])\n",
    "\n",
    "# get the most common words\n",
    "print(model.wv.index2word[0], model.wv.index2word[1], model.wv.index2word[2])\n",
    "\n",
    "# get the least common words\n",
    "vocab_size = len(model.wv.vocab)\n",
    "print(model.wv.index2word[vocab_size - 1], model.wv.index2word[vocab_size - 2], model.wv.index2word[vocab_size - 3])\n",
    "\n",
    "# find the index of the 2nd most common word (\"of\")\n",
    "print('Index of \"of\" is: {}'.format(model.wv.vocab['of'].index))\n",
    "\n",
    "# some similarity fun\n",
    "print(model.wv.similarity('woman', 'man'), model.wv.similarity('man', 'elephant'))\n",
    "\n",
    "# what doesn't fit?\n",
    "print(model.wv.doesnt_match(\"green blue red zebra\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
