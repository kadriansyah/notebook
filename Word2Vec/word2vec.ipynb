{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install numpy pandas matplotlib sklearn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing  import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "\n",
    "    \"\"\"\n",
    "    >>> Counter('abracadabra').most_common(3)\n",
    "    [('a', 5), ('r', 2), ('b', 2)]\n",
    "    note: https://kite.com/python/docs/collections.Counter.most_common\n",
    "    \"\"\"\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    \n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def collect_data(vocabulary_size=10000):\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = maybe_download('text8.zip', url, 31344016)\n",
    "    vocabulary = read_data(filename)\n",
    "    print(vocabulary[:10])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "[5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "['/device:CPU:0', '/device:XLA_CPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(get_available_devices())\n",
    "\n",
    "with tf.device('/device:CPU:0'):\n",
    "    window_size = 3\n",
    "    vector_dim = 300\n",
    "    epochs = 200000\n",
    "\n",
    "    valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "    sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "    couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "    word_target, word_context = zip(*couples)\n",
    "    word_target  = np.array(word_target, dtype=\"int32\")\n",
    "    word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "    print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    # create some input variables\n",
    "    input_target  = Input((1,))\n",
    "    input_context = Input((1,))\n",
    "\n",
    "    embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "    target  = embedding(input_target)\n",
    "    target  = Reshape((vector_dim, 1))(target)\n",
    "    context = embedding(input_context)\n",
    "    context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "    # setup a cosine similarity operation which will be output in a secondary model\n",
    "    # normalize: Whether to L2-normalize samples along the dot product axis before taking the dot product.\n",
    "    # If set to True, then the output of the dot product is the cosine proximity between the two samples.\n",
    "    similarity  = Dot(name=\"Cosine-Similarity\", axes=1, normalize=True)([target, context])\n",
    "\n",
    "    # now perform the dot product operation to get a similarity measure\n",
    "    dot_product = Dot(name=\"Dot-Product\", axes=1)([target, context])\n",
    "    dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "    # add the sigmoid output layer\n",
    "    output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "    # create the primary training model\n",
    "    model = Model([input_target, input_context], output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    # create a secondary validation model to run our similarity checks during training\n",
    "    validation_model = Model([input_target, input_context], similarity)\n",
    "\n",
    "    class SimilarityCallback:\n",
    "        def run_sim(self):\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                sim = self._get_sim(valid_examples[i])\n",
    "                nearest = (-sim).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "\n",
    "        @staticmethod\n",
    "        def _get_sim(valid_word_idx):\n",
    "            sim = np.zeros((vocab_size,))\n",
    "            in_arr1 = np.zeros((1,))\n",
    "            in_arr2 = np.zeros((1,))\n",
    "            in_arr1[0,] = valid_word_idx\n",
    "            for i in range(vocab_size):\n",
    "                in_arr2[0,] = i\n",
    "                out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "                sim[i] = out\n",
    "            return sim\n",
    "    sim_cb = SimilarityCallback()\n",
    "\n",
    "    arr_1 = np.zeros((1,))\n",
    "    arr_2 = np.zeros((1,))\n",
    "    arr_3 = np.zeros((1,))\n",
    "    for cnt in range(epochs):\n",
    "        idx = np.random.randint(0, len(labels)-1)\n",
    "        arr_1[0,] = word_target[idx]\n",
    "        arr_2[0,] = word_context[idx]\n",
    "        arr_3[0,] = labels[idx]\n",
    "        loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "        if cnt % 100 == 0:\n",
    "            print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "        if cnt % 10000 == 0:\n",
    "            sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
